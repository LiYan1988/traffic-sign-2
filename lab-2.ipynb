{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "with open('traffic.pickle', 'rb') as pfile:\n",
    "    data = pickle.load(pfile)\n",
    "    features = data['features']\n",
    "    labels = data['labels']\n",
    "    del data\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(features, labels, test_size=0.1, random_state=0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet(x):\n",
    "    # input: 32x32x3, output: 14x14x6\n",
    "    conv1_w = tf.Variable(tf.truncated_normal((5, 5, 3, 6), stddev=0.1))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1 = tf.nn.conv2d(x, conv1_w, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # input 14x14x6, output: 5x5x16\n",
    "    conv2_w = tf.Variable(tf.truncated_normal((5, 5, 6, 16), stddev=0.1))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2 = tf.nn.conv2d(conv1, conv2_w, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # fc 400\n",
    "    fc0 = tf.contrib.layers.flatten(conv2)\n",
    "    \n",
    "    # input:400, output: 120\n",
    "    fc1_w = tf.Variable(tf.truncated_normal((400, 120), stddev=0.1))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1 = tf.nn.relu(tf.matmul(fc0, fc1_w) + fc1_b)\n",
    "    fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "    \n",
    "    # input:120, output: 84\n",
    "    fc2_w = tf.Variable(tf.truncated_normal((120, 84), stddev=0.1))\n",
    "    fc2_b = tf.Variable(tf.zeros(84))\n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, fc2_w) + fc2_b)\n",
    "    fc2 = tf.nn.dropout(fc2, 0.5)\n",
    "    \n",
    "    # input: 84, output: n_labels, 43\n",
    "    fc3_w = tf.Variable(tf.truncated_normal((84, n_labels), stddev=0.1))\n",
    "    fc3_b = tf.Variable(tf.zeros(n_labels))\n",
    "    logits = tf.matmul(fc2, fc3_w) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "y_hot = tf.one_hot(y, n_labels)\n",
    "\n",
    "logits = lenet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_hot, logits=logits)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_hot, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    batch_size = 1000\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, batch_size):\n",
    "        batch_x, batch_y = X_data[offset:offset+batch_size], y_data[offset:offset+batch_size]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "def train_nn(epochs, batch_size, learning_rate):    \n",
    "    \n",
    "    validation_accuracy = []\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        num_examples = len(x_train)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            x_train0, y_train0 = shuffle(x_train, y_train)\n",
    "            for offset in range(0, num_examples, batch_size):\n",
    "                end = offset + batch_size\n",
    "                batch_x, batch_y = x_train0[offset:end], y_train0[offset:end]\n",
    "                sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "            acc = evaluate(x_valid, y_valid)\n",
    "#             print('Validation accuracy in epoch {:>}/{}: {:>.5f}'.format(i, epochs, acc))\n",
    "            validation_accuracy.append(acc)\n",
    "            \n",
    "    return validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nn():\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "        test_accuracy = evaluate(x_test, y_test)\n",
    "\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs = 50, batch_size = 128, learning_rate = 0.001: accuracy = 0.8370\n",
      "epochs = 50, batch_size = 128, learning_rate = 0.002: accuracy = 0.8564\n",
      "epochs = 50, batch_size = 128, learning_rate = 0.004: accuracy = 0.4685\n"
     ]
    }
   ],
   "source": [
    "epochs = [50, 100, 150]\n",
    "batch_sizes = [128, 256, 512, 1024, 2048]\n",
    "learning_rates = [0.001, 0.002, 0.004, 0.008, 0.02]\n",
    "validation_accuracy = {}\n",
    "for epoch in epochs:\n",
    "    for batch_size in batch_sizes:\n",
    "        for learning_rate in learning_rates:\n",
    "            validation_accuracy[epoch, batch_size, learning_rate] = train_nn(epoch, batch_size, learning_rate)\n",
    "            print('epochs = {}, batch_size = {}, learning_rate = {}: accuracy = {:>.4f}'.\\\n",
    "                  format(epoch, batch_size, learning_rate, validation_accuracy[epoch, batch_size, learning_rate][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_accuracy = test_nn()\n",
    "# print(test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
